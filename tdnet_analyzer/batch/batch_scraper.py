#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¥­ç¨®åˆ¥ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½
æ—¥ä»˜ç¯„å›²æŒ‡å®šã§æ¥­ç¨®ãƒ»è¦æ¨¡åˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
"""

import os
import tempfile
import argparse
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set
from google.cloud import storage
import json
import pytz
from concurrent.futures import ThreadPoolExecutor, as_completed

from tdnet_analyzer.scraper.tdnet_cloud import TDNetCloud
from tdnet_analyzer.common.path_utils import project_path
from tdnet_analyzer.common.constants import (
    load_company_info_map, normalize_code, normalize_size as normalize_size_const
)


def normalize_size(size):
    """è¦æ¨¡åˆ†é¡ã‚’æ­£è¦åŒ–ï¼ˆUnknown/ç©º/-ã¯otherã«ï¼‰"""
    if not size or size.strip() in ['', '-', 'Unknown', 'unknown', 'UNKNOWN']:
        return 'other'
    return size


def normalize_filename(code, company, title):
    """æ¥­ç¨®åˆ¥ä¿å­˜ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ"""
    return f"{code}_{company}_{title}.pdf"


class BatchScraper(TDNetCloud):
    """æ¥­ç¨®åˆ¥ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½"""

    def __init__(self, config_path: str | None = None, use_multithread: bool = True):
        super().__init__(config_path, use_multithread)
        # ä¼æ¥­æƒ…å ±ãƒãƒƒãƒ—èª­ã¿è¾¼ã¿
        self.company_info_map: Dict[str, tuple[str, str, str]] = {}
        companies_csv_path = project_path('inputs', 'companies.csv')
        if companies_csv_path.exists():
            try:
                self.company_info_map = load_company_info_map(str(companies_csv_path))
                self.logger.info(f"ä¼æ¥­æƒ…å ±ã‚’èª­ã¿è¾¼ã¿: {len(self.company_info_map)} ä»¶")
            except Exception as e:
                self.logger.warning(f"ä¼æ¥­æƒ…å ±ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        else:
            self.logger.warning("ä¼æ¥­æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    def _get_company_info(self, code: str) -> tuple[str, str, str]:
        """è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ä¼æ¥­æƒ…å ±ã‚’å–å¾—"""
        normalized_code = normalize_code(code)
        if normalized_code in self.company_info_map:
            name, sector, size = self.company_info_map[normalized_code]
            # è¦æ¨¡åˆ†é¡ã‚’æ­£è¦åŒ–
            normalized_size = normalize_size(size)
            return name, sector, normalized_size
        return 'Unknown', 'Unknown', 'other'

    def _generate_sector_gcs_path(self, data: Dict, sector: str, size: str) -> str:
        """æ¥­ç¨®åˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã§ã®GCSãƒ‘ã‚¹ç”Ÿæˆ"""
        company_code = data.get('code', 'unknown')
        company_name = data.get('company_name', 'Unknown')
        title = data.get('title', 'unknown')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã®å®‰å…¨åŒ–
        import re as _re
        safe_title = _re.sub(r'[<>:"/\\|?*]', '', title)
        safe_title = safe_title.replace(' ', '_')[:50].rstrip('_')
        safe_company_name = _re.sub(r'[<>:"/\\|?*]', '', company_name)
        safe_company_name = safe_company_name.replace(' ', '_')[:30].rstrip('_')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å: è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰_ä¼šç¤¾å_å…ƒãƒ•ã‚¡ã‚¤ãƒ«å.pdf
        filename = normalize_filename(company_code, safe_company_name, safe_title)
        
        # ãƒ‘ã‚¹: sectors/æ¥­ç¨®/è¦æ¨¡/filename
        path_parts = [self.base_path, 'sectors', sector, size, filename]
        return "/".join(part for part in path_parts if part)

    def _process_documents_sector_batch(self, data_list: List[Dict]) -> int:
        """æ¥­ç¨®åˆ¥ãƒãƒƒãƒå‡¦ç†ã§PDFã‚’å‡¦ç†"""
        if not self.use_multithread:
            return self._process_documents_sector_single(data_list)
        
        self.logger.info(f"æ¥­ç¨®åˆ¥ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†é–‹å§‹ (ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.max_workers})")

        def process_single_document(data):
            try:
                pdf_url = data.get('pdf_url')
                if not pdf_url:
                    return False, f"PDF URLãŒå­˜åœ¨ã—ã¾ã›ã‚“: {data.get('title', 'Unknown')}"
                
                # ä¼æ¥­æƒ…å ±å–å¾—
                code = data.get('code', 'unknown')
                company_name, sector, size = self._get_company_info(code)
                
                # PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                temp_file_path = self._download_pdf_to_temp(pdf_url)
                if not temp_file_path:
                    return False, f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {data.get('title', 'Unknown')}"
                
                try:
                    # æ¥­ç¨®åˆ¥GCSãƒ‘ã‚¹ç”Ÿæˆ
                    gcs_path = self._generate_sector_gcs_path(data, sector, size)
                    if self._upload_to_gcs(temp_file_path, gcs_path):
                        return True, f"å‡¦ç†å®Œäº†: {gcs_path} (æ¥­ç¨®:{sector}, è¦æ¨¡:{size})"
                    else:
                        return False, f"GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {data.get('title', 'Unknown')}"
                finally:
                    if os.path.exists(temp_file_path):
                        os.unlink(temp_file_path)
            except Exception as e:
                return False, f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"

        success_count = 0
        processed_count = 0
        total_count = len(data_list)
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_data = {executor.submit(process_single_document, data): data for data in data_list}
            for future in as_completed(future_to_data):
                data = future_to_data[future]
                processed_count += 1
                try:
                    success, message = future.result()
                    if success:
                        success_count += 1
                        code = data.get('code', 'unknown')
                        company_name, sector, size = self._get_company_info(code)
                        self.logger.info(
                            f"âœ… [{processed_count}/{total_count}] æˆåŠŸ: code={code} æ¥­ç¨®={sector} è¦æ¨¡={size} title=\"{data.get('title', 'Unknown')[:30]}...\""
                        )
                    else:
                        self.logger.error(
                            f"âŒ [{processed_count}/{total_count}] å¤±æ•—: code={data.get('code')} title=\"{data.get('title', 'Unknown')[:30]}...\" - {message}"
                        )
                except Exception as e:
                    self.logger.error(f"ğŸ’¥ [{processed_count}/{total_count}] å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                
                if processed_count % 10 == 0 or processed_count == total_count:
                    progress_rate = (processed_count / total_count) * 100
                    self.logger.info(f"ğŸ“Š é€²æ—: {processed_count}/{total_count} ä»¶å®Œäº† ({progress_rate:.1f}%) - æˆåŠŸ: {success_count} ä»¶")
        
        self.logger.info(f"æ¥­ç¨®åˆ¥ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†å®Œäº†: {success_count}/{len(data_list)} ä»¶æˆåŠŸ")
        return success_count

    def _process_documents_sector_single(self, data_list: List[Dict]) -> int:
        """æ¥­ç¨®åˆ¥ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†"""
        self.logger.info("æ¥­ç¨®åˆ¥ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†é–‹å§‹")
        success_count = 0
        
        for data in data_list:
            try:
                pdf_url = data.get('pdf_url')
                if not pdf_url:
                    self.logger.warning(f"PDF URLãŒå­˜åœ¨ã—ã¾ã›ã‚“: {data.get('title', 'Unknown')}")
                    continue
                
                # ä¼æ¥­æƒ…å ±å–å¾—
                code = data.get('code', 'unknown')
                company_name, sector, size = self._get_company_info(code)
                
                # PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                temp_file_path = self._download_pdf_to_temp(pdf_url)
                if not temp_file_path:
                    self.logger.error(f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {data.get('title', 'Unknown')}")
                    continue
                
                try:
                    # æ¥­ç¨®åˆ¥GCSãƒ‘ã‚¹ç”Ÿæˆ
                    gcs_path = self._generate_sector_gcs_path(data, sector, size)
                    if self._upload_to_gcs(temp_file_path, gcs_path):
                        success_count += 1
                        self.logger.info(
                            f"æˆåŠŸ: code={code} æ¥­ç¨®={sector} è¦æ¨¡={size} title=\"{data.get('title', 'Unknown')}\" -> {gcs_path}"
                        )
                    else:
                        self.logger.error(f"GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {data.get('title', 'Unknown')}")
                finally:
                    if os.path.exists(temp_file_path):
                        os.unlink(temp_file_path)
            except Exception as e:
                self.logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        
        self.logger.info(f"æ¥­ç¨®åˆ¥ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†å®Œäº†: {success_count}/{len(data_list)} ä»¶æˆåŠŸ")
        return success_count

    def scrape_date_range_sector_batch(self, start_date: str, end_date: str) -> Dict[str, int]:
        """æ—¥ä»˜ç¯„å›²ã§æ¥­ç¨®åˆ¥ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        self.logger.info(f"æ¥­ç¨®åˆ¥ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {start_date} - {end_date}")
        
        results = {}
        current_date = datetime.strptime(start_date, '%Y%m%d')
        end_date_obj = datetime.strptime(end_date, '%Y%m%d')
        
        # å„æ—¥ä»˜ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        all_data = []
        
        while current_date <= end_date_obj:
            date_str = current_date.strftime('%Y%m%d')
            self.logger.info(f"æ—¥ä»˜ {date_str} ã®ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
            
            if not self._check_date_exists(date_str):
                self.logger.warning(f"æ—¥ä»˜ {date_str} ã®ãƒ‡ãƒ¼ã‚¿ã¯å­˜åœ¨ã—ã¾ã›ã‚“")
                results[date_str] = 0
                current_date += timedelta(days=1)
                continue
            
            # ãã®æ—¥ã®ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
            page_data_for_date = []
            page_index = 1
            
            while True:
                page_data = self.scrape_page(page_index, date_str)
                if not page_data:
                    break
                page_data_for_date.extend(page_data)
                page_index += 1
            
            # å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
            if self.company_market_map and self.excluded_markets:
                before = len(page_data_for_date)
                
                def _is_excluded_by_market(data: Dict) -> bool:
                    code_raw = str(data.get('code', '')).strip().upper()
                    code_norm = normalize_code(code_raw)
                    market = (
                        self.company_market_map.get(code_raw)
                        or self.company_market_map.get(code_norm)
                    )
                    return market in self.excluded_markets
                
                page_data_for_date = [d for d in page_data_for_date if not _is_excluded_by_market(d)]
                excluded = before - len(page_data_for_date)
                if excluded:
                    self.logger.info(f"æ—¥ä»˜ {date_str}: å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿ã§ {excluded}ä»¶ã‚’é™¤å¤–")
            
            all_data.extend(page_data_for_date)
            results[date_str] = len(page_data_for_date)
            self.logger.info(f"æ—¥ä»˜ {date_str}: {len(page_data_for_date)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†")
            
            current_date += timedelta(days=1)
        
        if not all_data:
            self.logger.warning("å¯¾è±¡æœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return results
        
        self.logger.info(f"å…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: åˆè¨ˆ {len(all_data)} ä»¶")
        
        # æ¥­ç¨®åˆ¥ãƒãƒƒãƒå‡¦ç†ã§PDFä¿å­˜
        self.logger.info("æ¥­ç¨®åˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã§ã®ä¸€æ‹¬ä¿å­˜é–‹å§‹...")
        saved_count = self._process_documents_sector_batch(all_data)
        
        self.logger.info(f"æ¥­ç¨®åˆ¥ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {saved_count}/{len(all_data)} ä»¶ä¿å­˜")
        
        # çµæœã‚µãƒãƒªãƒ¼
        total_collected = sum(results.values())
        self.logger.info(f"æœŸé–“åˆ¥åé›†ã‚µãƒãƒªãƒ¼:")
        for date, count in results.items():
            self.logger.info(f"  {date}: {count} ä»¶")
        self.logger.info(f"åˆè¨ˆåé›†: {total_collected} ä»¶, ä¿å­˜æˆåŠŸ: {saved_count} ä»¶")
        
        return results


def main():
    parser = argparse.ArgumentParser(description='æ¥­ç¨®åˆ¥ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°')
    parser.add_argument('--start-date', type=str, required=True, help='é–‹å§‹æ—¥ä»˜ (YYYYMMDDå½¢å¼)')
    parser.add_argument('--end-date', type=str, required=True, help='çµ‚äº†æ—¥ä»˜ (YYYYMMDDå½¢å¼)')
    parser.add_argument('--key-file', type=str, help='GCPã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--project-id', type=str, help='GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID')
    parser.add_argument('--single-thread', action='store_true', help='ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--config', type=str, default=str(project_path('config', 'config.yaml')), help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    
    args = parser.parse_args()
    
    # GCPèªè¨¼è¨­å®š
    if args.key_file:
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = args.key_file
    if args.project_id:
        os.environ['PROJECT_ID'] = args.project_id
    
    try:
        scraper = BatchScraper(args.config, use_multithread=not args.single_thread)
        results = scraper.scrape_date_range_sector_batch(args.start_date, args.end_date)
        
        total_count = sum(results.values())
        print(f"æ¥­ç¨®åˆ¥ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: åˆè¨ˆ {total_count} ä»¶ã®æ–‡æ›¸ã‚’å‡¦ç†")
        for date, count in results.items():
            print(f"  {date}: {count} ä»¶")
        
        return 0
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())