#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½
æ—¢å­˜ã‚¯ãƒ©ã‚¦ãƒ‰ç‰ˆã¨åŒã˜ä»•æ§˜ã§ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ä¿å­˜
"""

import os
import tempfile
import argparse
import json
import shutil
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Set

from tdnet_analyzer.scraper.tdnet_base import TDNetBase
from tdnet_analyzer.common.path_utils import project_path
from tdnet_analyzer.common.constants import (
    load_company_market_map, EXCLUDED_MARKETS_DEFAULT, normalize_code
)


class TDNetLocal(TDNetBase):
    """ãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆã®é©æ™‚é–‹ç¤ºãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""

    def __init__(self, config_path: str | None = None, use_multithread: bool = True, out_root: str = "./local_store"):
        super().__init__(config_path, use_multithread)
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜è¨­å®š
        self.out_root = Path(out_root)
        self.out_root.mkdir(parents=True, exist_ok=True)
        
        # å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿ã®èª­ã¿è¾¼ã¿
        self.company_market_map: Dict[str, str] = {}
        self.excluded_markets: Set[str] = EXCLUDED_MARKETS_DEFAULT
        companies_csv_path = project_path('inputs', 'companies.csv')
        if companies_csv_path.exists():
            try:
                self.company_market_map = load_company_market_map(str(companies_csv_path))
                self.logger.info(f"å¸‚å ´å®šç¾©ã‚’èª­ã¿è¾¼ã¿: {len(self.company_market_map)} ä»¶")
            except Exception as e:
                self.logger.warning(f"å¸‚å ´å®šç¾©ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        else:
            self.logger.warning("å¸‚å ´å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ•ã‚£ãƒ«ã‚¿ã¯ç„¡åŠ¹")
        
        self.logger.info(f"é™¤å¤–å¯¾è±¡å¸‚å ´: {self.excluded_markets}")
        self.logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆ: {self.out_root}")

    def _download_pdf_to_local(self, pdf_url: str, local_path: str) -> bool:
        """PDFã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            self._rate_limit_wait()
            response = self.session.get(pdf_url, timeout=30, verify=False)
            if response.status_code == 200:
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
                os.makedirs(os.path.dirname(local_path), exist_ok=True)
                with open(local_path, 'wb') as f:
                    f.write(response.content)
                self.logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å®Œäº†: {local_path}")
                return True
            else:
                self.logger.error(f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {response.status_code} - {pdf_url}")
                return False
        except Exception as e:
            self.logger.error(f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _generate_local_path(self, data: Dict, date_str: str) -> str:
        """ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ãƒ‘ã‚¹ç”Ÿæˆï¼ˆæ—¢å­˜ã‚¯ãƒ©ã‚¦ãƒ‰ç‰ˆã¨åŒã˜æ§‹é€ ï¼‰"""
        year = date_str[:4]
        month = date_str[4:6]
        day = date_str[6:8]
        
        # å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨ï¼ˆæ—¢å­˜ã‚¯ãƒ©ã‚¦ãƒ‰ç‰ˆã¨åŒã˜ï¼‰
        title = data.get('title', 'unknown')
        import re as _re
        safe_title = _re.sub(r'[<>:"/\\|?*]', '', title)
        safe_title = safe_title.replace(' ', '_')[:100].rstrip('_')
        filename = f"{safe_title}.pdf"
        
        # ãƒ‘ã‚¹æ§‹é€ : out_root/YYYY/MM/DD/filename
        local_path = self.out_root / year / month / day / filename
        return str(local_path)

    def _process_documents_local(self, data_list: List[Dict], date_str: str) -> int:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰ã§PDFå‡¦ç†"""
        if not self.use_multithread:
            return self._process_documents_single_local(data_list, date_str)
        
        self.logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§PDFå‡¦ç†é–‹å§‹ (ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.max_workers})")
        
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        def process_single_document(data):
            try:
                pdf_url = data.get('pdf_url')
                if not pdf_url:
                    return False, f"PDF URLãŒå­˜åœ¨ã—ã¾ã›ã‚“: {data.get('title', 'Unknown')}"
                
                local_path = self._generate_local_path(data, date_str)
                if self._download_pdf_to_local(pdf_url, local_path):
                    return True, f"å‡¦ç†å®Œäº†: {local_path}"
                else:
                    return False, f"ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å¤±æ•—: {data.get('title', 'Unknown')}"
            except Exception as e:
                return False, f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"

        success_count = 0
        processed_count = 0
        total_count = len(data_list)
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_data = {executor.submit(process_single_document, data): data for data in data_list}
            for future in as_completed(future_to_data):
                data = future_to_data[future]
                processed_count += 1
                try:
                    success, message = future.result()
                    if success:
                        success_count += 1
                        self.logger.info(
                            f"âœ… [{processed_count}/{total_count}] æˆåŠŸ: code={data.get('code')} type={data.get('doc_type')} title=\"{data.get('title', 'Unknown')[:50]}...\""
                        )
                    else:
                        self.logger.error(
                            f"âŒ [{processed_count}/{total_count}] å¤±æ•—: code={data.get('code')} title=\"{data.get('title', 'Unknown')[:50]}...\" - {message}"
                        )
                except Exception as e:
                    self.logger.error(f"ğŸ’¥ [{processed_count}/{total_count}] å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                
                if processed_count % 10 == 0 or processed_count == total_count:
                    progress_rate = (processed_count / total_count) * 100
                    self.logger.info(f"ğŸ“Š é€²æ—: {processed_count}/{total_count} ä»¶å®Œäº† ({progress_rate:.1f}%) - æˆåŠŸ: {success_count} ä»¶")
        
        self.logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†å®Œäº†: {success_count}/{len(data_list)} ä»¶æˆåŠŸ")
        return success_count

    def _process_documents_single_local(self, data_list: List[Dict], date_str: str) -> int:
        """ãƒ­ãƒ¼ã‚«ãƒ«ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†"""
        self.logger.info("ãƒ­ãƒ¼ã‚«ãƒ«ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†é–‹å§‹")
        success_count = 0
        
        for data in data_list:
            try:
                pdf_url = data.get('pdf_url')
                if not pdf_url:
                    self.logger.warning(f"PDF URLãŒå­˜åœ¨ã—ã¾ã›ã‚“: {data.get('title', 'Unknown')}")
                    continue
                
                local_path = self._generate_local_path(data, date_str)
                if self._download_pdf_to_local(pdf_url, local_path):
                    success_count += 1
                    self.logger.info(
                        f"æˆåŠŸ: code={data.get('code')} type={data.get('doc_type')} title=\"{data.get('title', 'Unknown')}\" -> {local_path}"
                    )
                else:
                    self.logger.error(f"ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å¤±æ•—: {data.get('title', 'Unknown')}")
            except Exception as e:
                self.logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        
        self.logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†å®Œäº†: {success_count}/{len(data_list)} ä»¶æˆåŠŸ")
        return success_count

    def _save_metadata_to_local(self, data_list: List[Dict], date_str: str):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆæ—¢å­˜ã‚¯ãƒ©ã‚¦ãƒ‰ç‰ˆã¨åŒã˜å½¢å¼ï¼‰"""
        try:
            year = date_str[:4]
            month = date_str[4:6]
            day = date_str[6:8]
            
            metadata = {
                'date': date_str,
                'total_documents': len(data_list),
                'document_types': {},
                'companies': {},
                'documents': []
            }
            
            for data in data_list:
                doc_type = data.get('doc_type', 'other')
                company_code = data.get('code', 'unknown')
                metadata['document_types'][doc_type] = metadata['document_types'].get(doc_type, 0) + 1
                metadata['companies'][company_code] = metadata['companies'].get(company_code, 0) + 1
                
                doc_info = {
                    'time': data.get('time'),
                    'code': data.get('code'),
                    'company_name': data.get('company_name'),
                    'title': data.get('title'),
                    'doc_type': data.get('doc_type'),
                    'local_path': self._generate_local_path(data, date_str)
                }
                metadata['documents'].append(doc_info)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            metadata_dir = self.out_root / year / month / day
            metadata_dir.mkdir(parents=True, exist_ok=True)
            metadata_path = metadata_dir / f"metadata_{date_str}.json"
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {metadata_path}")
            
        except Exception as e:
            self.logger.error(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    def scrape_date(self, date_str: str) -> int:
        """æ—¥ä»˜æŒ‡å®šã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆæ—¢å­˜ã‚¯ãƒ©ã‚¦ãƒ‰ç‰ˆã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
        if not self._check_date_exists(date_str):
            self.logger.warning(f"æ—¥ä»˜ {date_str} ã®ãƒ‡ãƒ¼ã‚¿ã¯å­˜åœ¨ã—ã¾ã›ã‚“")
            return 0
        
        total_saved = 0
        page_index = 1
        batch_size = 50
        
        while True:
            page_data = self.scrape_page(page_index, date_str)
            if not page_data:
                self.logger.info(f"ãƒšãƒ¼ã‚¸ {page_index} ã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’çµ‚äº†")
                break
            
            self.logger.info(f"å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨å‰: {len(page_data)}ä»¶ã®æ–‡æ›¸")
            
            # å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
            if self.company_market_map and self.excluded_markets:
                before = len(page_data)

                def _is_excluded_by_market(data: Dict) -> bool:
                    code_raw = str(data.get('code', '')).strip().upper()
                    code_norm = normalize_code(code_raw)
                    market = (
                        self.company_market_map.get(code_raw)
                        or self.company_market_map.get(code_norm)
                    )
                    is_excluded = market in self.excluded_markets
                    if is_excluded:
                        self.logger.info(f"  é™¤å¤–å¯¾è±¡: ã‚³ãƒ¼ãƒ‰={code_raw} (æ­£è¦åŒ–å¾Œ={code_norm}), å¸‚å ´={market}")
                    else:
                        self.logger.debug(f"  é™¤å¤–å¯¾è±¡å¤–: ã‚³ãƒ¼ãƒ‰={code_raw} (æ­£è¦åŒ–å¾Œ={code_norm}), å¸‚å ´={market}")
                    return is_excluded
                
                page_data = [d for d in page_data if not _is_excluded_by_market(d)]
                excluded = before - len(page_data)
                if excluded:
                    self.logger.info(f"å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿: {excluded}ä»¶ã‚’é™¤å¤–ï¼ˆå¯¾è±¡å¸‚å ´: {', '.join(sorted(self.excluded_markets))}ï¼‰")
                self.logger.info(f"ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å¯¾è±¡ä»¶æ•°: {len(page_data)}ä»¶")
            
            if len(page_data) >= batch_size:
                self.logger.info(f"ãƒšãƒ¼ã‚¸ {page_index}: {len(page_data)}ä»¶ã‚’ãƒãƒƒãƒå‡¦ç†ä¸­...")
                saved_count = self._process_documents_local(page_data, date_str)
                total_saved += saved_count
                self.logger.info(f"ãƒšãƒ¼ã‚¸ {page_index} å®Œäº†: {saved_count}ä»¶ä¿å­˜")
                page_data.clear()
            
            page_index += 1
        
        if page_data:
            self.logger.info(f"æœ€çµ‚ãƒãƒƒãƒ: {len(page_data)}ä»¶ã‚’å‡¦ç†ä¸­...")
            saved_count = self._process_documents_local(page_data, date_str)
            total_saved += saved_count
            self.logger.info(f"æœ€çµ‚ãƒãƒƒãƒå®Œäº†: {saved_count}ä»¶ä¿å­˜")
        
        if total_saved > 0:
            self.logger.info(f"åˆè¨ˆ {total_saved} ä»¶ã®æ–‡æ›¸ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ")
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            self._save_metadata_to_local(self._get_all_metadata(date_str), date_str)
            self.logger.info(f"å®Œäº†: æ—¥ä»˜ {date_str}, ä¿å­˜ä»¶æ•°: {total_saved}")
            return total_saved
        else:
            self.logger.warning("å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            self.logger.info(f"å®Œäº†: æ—¥ä»˜ {date_str}, ä¿å­˜ä»¶æ•°: 0")
            return 0

    def _get_all_metadata(self, date_str: str) -> List[Dict]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—ï¼ˆæ—¢å­˜ã‚¯ãƒ©ã‚¦ãƒ‰ç‰ˆã¨åŒã˜ï¼‰"""
        self.logger.info("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—ä¸­...")
        all_data = []
        page_index = 1
        
        while True:
            page_data = self.scrape_page(page_index, date_str)
            if not page_data:
                break
            all_data.extend(page_data)
            page_index += 1
        
        return all_data


def main():
    parser = argparse.ArgumentParser(description='ãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆTDnetã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°')
    parser.add_argument('--date', type=str, required=True, help='å¯¾è±¡æ—¥ä»˜ (YYYYMMDDå½¢å¼)')
    parser.add_argument('--out-root', type=str, default='./local_store', help='ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--single-thread', action='store_true', help='ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--config', type=str, default=str(project_path('config', 'config.yaml')), help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    
    args = parser.parse_args()
    
    try:
        scraper = TDNetLocal(args.config, use_multithread=not args.single_thread, out_root=args.out_root)
        count = scraper.scrape_date(args.date)
        print(f"ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {count} ä»¶ã®æ–‡æ›¸ã‚’ä¿å­˜")
        return 0
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())