#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥æœ¬æ ªã®é©æ™‚é–‹ç¤ºã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã€GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€
TDnetãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç”¨ã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’ä½œæˆ
"""

import os
import tempfile
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set
import argparse
from google.cloud import storage
import json
import pytz
from concurrent.futures import ThreadPoolExecutor, as_completed
import functions_framework
import re

import google.auth
import google.auth.transport.requests
import requests

from tdnet_base import TDNetBase
# å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿ç”¨
try:
    from constants import load_company_market_map, EXCLUDED_MARKETS_DEFAULT, normalize_code
except Exception:
    load_company_market_map = None
    EXCLUDED_MARKETS_DEFAULT = set()

    def normalize_code(c: str) -> str:
        return c


def _extract_date_from_request(request) -> str | None:
    """HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ 'date' (YYYYMMDD) ã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
    if request is None:
        return None

    # 1) JSON body
    try:
        data = request.get_json(silent=True)
        if isinstance(data, dict):
            body_date = data.get('date')
            if body_date and re.match(r'^\d{8}$', body_date):
                return body_date
    except Exception:
        pass

    # 2) query string
    try:
        if getattr(request, 'args', None):
            query_date = request.args.get('date')
            if query_date and re.match(r'^\d{8}$', query_date):
                return query_date
    except Exception:
        pass

    return None


@functions_framework.http
def scrape(request):
    """
    Cloud Functionã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã€‚
    HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã®TDnetã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    æ—¥ä»˜ãŒæŒ‡å®šã•ã‚Œãªã„å ´åˆã¯ã€JSTã®ç¾åœ¨ã®æ—¥ä»˜ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
    """
    try:
        date_str = _extract_date_from_request(request)
        if not date_str:
            jst = pytz.timezone('Asia/Tokyo')
            date_str = datetime.now(jst).strftime('%Y%d')
            print(f"æ—¥ä»˜ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ç¾åœ¨ã®æ—¥ä»˜ã‚’ä½¿ç”¨ã—ã¾ã™: {date_str}")

        print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™: æ—¥ä»˜={date_str}")

        scraper = TDNetCloud(config_path="config/config.yaml", use_multithread=True)
        count = scraper.scrape_date(date_str)

        message = f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ: {count} ä»¶ã®æ–‡æ›¸ã‚’ä¿å­˜"
        print(message)
        return {"status": "success", "message": message, "count": count}, 200

    except Exception as e:
        import traceback
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(traceback.format_exc())
        return {"status": "error", "message": str(e)}, 500


class TDNetCloud(TDNetBase):
    """Google Cloudç”¨ã®é©æ™‚é–‹ç¤ºãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""

    def __init__(self, config_path: str = "config/config.yaml", use_multithread: bool = True):
        """åˆæœŸåŒ–"""
        super().__init__(config_path, use_multithread)

        # --- GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º ---
        try:
            credentials, project_id = google.auth.default()

            # ä¸¦åˆ—æ•°ã«å¿œã˜ã¦æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
            adapter = requests.adapters.HTTPAdapter(pool_connections=self.max_workers, pool_maxsize=self.max_workers)
            session = requests.Session()
            session.mount('https://', adapter)

            authed_session = google.auth.transport.requests.AuthorizedSession(credentials, session=session)

            self.storage_client = storage.Client(project=project_id, credentials=credentials, _http=authed_session)
            self.logger.info(f"GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project_id}, æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: {self.max_workers}")
        except Exception as e:
            self.logger.warning(f"GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã«å¤±æ•—ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ç¶šè¡Œã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã‚‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’æŒ‡å®šã™ã‚‹
            try:
                credentials, project_id = google.auth.default()
                self.storage_client = storage.Client(project=project_id)
            except Exception:
                self.storage_client = storage.Client(project=os.environ.get("PROJECT_ID"))

        # GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã¿ä½¿ç”¨

        # è¨­å®šã‹ã‚‰GCSè¨­å®šã‚’èª­ã¿è¾¼ã¿
        gcs_config = self.config.get('gcs', {})
        self.bucket_name = gcs_config.get('bucket_name')
        self.base_path = gcs_config.get('base_path', 'tdnet-analyzer')
        # doc_typeå¼·åˆ¶ï¼ˆä¾‹: "tanshin"ï¼‰ã€‚æœªè¨­å®šãªã‚‰åˆ†é¡çµæœã‚’ä½¿ç”¨
        self.force_doc_type = gcs_config.get('force_doc_type')
        # ãƒ•ãƒ©ãƒƒãƒˆä¿å­˜è¨­å®š
        self.flat_per_day = bool(gcs_config.get('flat_per_day', False))

        if not self.bucket_name:
            raise ValueError("GCSãƒã‚±ãƒƒãƒˆåãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®šã®ç¢ºèªã¨ãƒ­ã‚°å‡ºåŠ›
        self.logger.info("ğŸš€ TDnet ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š:")
        self.logger.info(f"  â€¢ GCSãƒã‚±ãƒƒãƒˆ: {self.bucket_name}")
        self.logger.info(f"  â€¢ ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰: {self.use_multithread}")
        self.logger.info(f"  â€¢ æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.max_workers}")
        self.logger.info(f"  â€¢ ãƒ•ãƒ©ãƒƒãƒˆä¿å­˜: {self.config.get('gcs', {}).get('flat_per_day', False)}")
        self.logger.info(f"  â€¢ ãƒ™ãƒ¼ã‚¹URL: {self.config.get('data_fetch', {}).get('base_url', 'æœªè¨­å®š')}")

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®šã‚’å‹•çš„ã«å–å¾—
        rate_limit_config = self.config.get('scraping', {}).get('rate_limit', {})
        max_requests = rate_limit_config.get('max_requests_per_second', 5)
        self.logger.info(f"  â€¢ ãƒ¬ãƒ¼ãƒˆåˆ¶é™: 1ç§’ã‚ãŸã‚Šæœ€å¤§{max_requests}ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")

        # å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿ã®èª­ã¿è¾¼ã¿
        self.company_market_map: Dict[str, str] = {}
        self.excluded_markets: Set[str] = EXCLUDED_MARKETS_DEFAULT
        companies_csv_path = os.path.join(os.path.dirname(__file__), 'inputs', 'companies.csv')
        if load_company_market_map and os.path.exists(companies_csv_path):
            try:
                self.company_market_map = load_company_market_map(companies_csv_path)
                self.logger.info(f"å¸‚å ´å®šç¾©ã‚’èª­ã¿è¾¼ã¿: {len(self.company_market_map)} ä»¶")
            except Exception as e:
                self.logger.warning(f"å¸‚å ´å®šç¾©ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        else:
            self.logger.info("å¸‚å ´å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ•ã‚£ãƒ«ã‚¿ã¯ç„¡åŠ¹")
        self.logger.info(f"é™¤å¤–å¯¾è±¡å¸‚å ´: {self.excluded_markets}")

    def _download_pdf_to_temp(self, pdf_url: str) -> Optional[str]:
        """PDFã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é©ç”¨
            self._rate_limit_wait()

            response = self.session.get(pdf_url, timeout=30, verify=False)
            if response.status_code == 200:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')
                temp_file.write(response.content)
                temp_file.close()
                return temp_file.name
            else:
                self.logger.error(f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {response.status_code} - {pdf_url}")
                return None
        except Exception as e:
            self.logger.error(f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _upload_to_gcs(self, local_file_path: str, gcs_path: str) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        try:
            bucket = self.storage_client.bucket(self.bucket_name)
            blob = bucket.blob(gcs_path)

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            blob.upload_from_filename(local_file_path)

            self.logger.info(f"GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {gcs_path}")
            return True

        except Exception as e:
            self.logger.error(f"GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _generate_gcs_path(self, data: Dict, date_str: str) -> str:
        """Vertex AI RAGç”¨ã®GCSãƒ‘ã‚¹ã‚’ç”Ÿæˆ"""
        year = date_str[:4]
        month = date_str[4:6]
        day = date_str[6:8]

        company_code = data.get('code', 'unknown')
        doc_type = data.get('doc_type', 'other')

        title = data.get('title', 'unknown')
        # æ—¥æœ¬èªæ–‡å­—ã‚‚ä¿æŒã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§å•é¡Œã¨ãªã‚‹æ–‡å­—ã®ã¿é™¤å»
        import re
        safe_title = re.sub(r'[<>:"/\\|?*]', '', title)  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§ç¦æ­¢ã•ã‚ŒãŸæ–‡å­—ã®ã¿é™¤å»
        safe_title = safe_title.replace(' ', '_')[:50].rstrip('_')

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯åˆ†ã‘ãšã€æ—¥ä»˜é…ä¸‹ã«ãƒ•ãƒ©ãƒƒãƒˆä¿å­˜
        if self.flat_per_day:
            filename = f"{company_code}_{safe_title}.pdf"
            return f"{self.base_path}/{year}/{month}/{day}/{filename}"

        # å¾“æ¥ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆdoc_typeé…ä¸‹ï¼‰
        filename = f"{company_code}_{safe_title}.pdf"
        return f"{self.base_path}/{year}/{month}/{day}/{doc_type}/{filename}"

    def _process_documents_cloud(self, data_list: List[Dict], date_str: str) -> int:
        """Cloudç”¨ã®æ–‡æ›¸å‡¦ç†ï¼ˆPDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ + GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰"""
        if not self.use_multithread:
            return self._process_documents_single_cloud(data_list, date_str)

        self.logger.info(f"ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§PDFå‡¦ç†é–‹å§‹ (ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.max_workers})")

        def process_single_document(data):
            """å˜ä¸€æ–‡æ›¸ã®å‡¦ç†"""
            try:
                pdf_url = data.get('pdf_url')
                if not pdf_url:
                    return False, f"PDF URLãŒå­˜åœ¨ã—ã¾ã›ã‚“: {data.get('title', 'Unknown')}"

                # PDFã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                temp_file_path = self._download_pdf_to_temp(pdf_url)
                if not temp_file_path:
                    return False, f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {data.get('title', 'Unknown')}"

                try:
                    # GCSãƒ‘ã‚¹ã‚’ç”Ÿæˆ
                    gcs_path = self._generate_gcs_path(data, date_str)

                    # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                    if self._upload_to_gcs(temp_file_path, gcs_path):
                        return True, f"å‡¦ç†å®Œäº†: {gcs_path}"
                    else:
                        return False, f"GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {data.get('title', 'Unknown')}"

                finally:
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    if os.path.exists(temp_file_path):
                        os.unlink(temp_file_path)

            except Exception as e:
                return False, f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"

        # ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§å‡¦ç†
        success_count = 0
        processed_count = 0
        total_count = len(data_list)

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_data = {executor.submit(process_single_document, data): data for data in data_list}

            for future in as_completed(future_to_data):
                data = future_to_data[future]
                processed_count += 1

                try:
                    success, message = future.result()
                    if success:
                        success_count += 1
                        self.logger.info(
                            f"âœ… [{processed_count}/{total_count}] æˆåŠŸ: code={data.get('code')} type={data.get('doc_type')} title=\"{data.get('title', 'Unknown')[:50]}...\""
                        )
                    else:
                        self.logger.error(
                            f"âŒ [{processed_count}/{total_count}] å¤±æ•—: code={data.get('code')} title=\"{data.get('title', 'Unknown')[:50]}...\" - {message}"
                        )
                except Exception as e:
                    self.logger.error(f"ğŸ’¥ [{processed_count}/{total_count}] å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

                # 10ä»¶ã”ã¨ã«é€²æ—ç‡ã‚’ãƒ­ã‚°å‡ºåŠ›
                if processed_count % 10 == 0 or processed_count == total_count:
                    progress_rate = (processed_count / total_count) * 100
                    self.logger.info(f"ğŸ“Š é€²æ—: {processed_count}/{total_count} ä»¶å®Œäº† ({progress_rate:.1f}%) - æˆåŠŸ: {success_count} ä»¶")

        self.logger.info(f"ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†å®Œäº†: {success_count}/{len(data_list)} ä»¶æˆåŠŸ")
        return success_count

    def _process_documents_single_cloud(self, data_list: List[Dict], date_str: str) -> int:
        """ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã®æ–‡æ›¸å‡¦ç†"""
        self.logger.info("ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§PDFå‡¦ç†é–‹å§‹")

        success_count = 0
        for data in data_list:
            try:
                pdf_url = data.get('pdf_url')
                if not pdf_url:
                    self.logger.warning(f"PDF URLãŒå­˜åœ¨ã—ã¾ã›ã‚“: {data.get('title', 'Unknown')}")
                    continue

                # PDFã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                temp_file_path = self._download_pdf_to_temp(pdf_url)
                if not temp_file_path:
                    self.logger.error(f"PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {data.get('title', 'Unknown')}")
                    continue

                try:
                    # GCSãƒ‘ã‚¹ã‚’ç”Ÿæˆ
                    gcs_path = self._generate_gcs_path(data, date_str)

                    # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                    if self._upload_to_gcs(temp_file_path, gcs_path):
                        success_count += 1
                        self.logger.info(
                            f"æˆåŠŸ: code={data.get('code')} type={data.get('doc_type')} title=\"{data.get('title', 'Unknown')}\" -> {gcs_path}"
                        )
                    else:
                        self.logger.error(f"GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {data.get('title', 'Unknown')}")

                finally:
                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    if os.path.exists(temp_file_path):
                        os.unlink(temp_file_path)

            except Exception as e:
                self.logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        self.logger.info(f"ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†å®Œäº†: {success_count}/{len(data_list)} ä»¶æˆåŠŸ")
        return success_count

    def _save_metadata_to_gcs(self, data_list: List[Dict], date_str: str):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’GCSã«ä¿å­˜ï¼ˆVertex AI RAGç”¨ï¼‰"""
        try:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–
            metadata = {
                'date': date_str,
                'total_documents': len(data_list),
                'document_types': {},
                'companies': {},
                'documents': []
            }

            # çµ±è¨ˆæƒ…å ±ã‚’é›†è¨ˆ
            for data in data_list:
                doc_type = data.get('doc_type', 'other')
                company_code = data.get('code', 'unknown')

                # æ–‡æ›¸ã‚¿ã‚¤ãƒ—åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
                metadata['document_types'][doc_type] = metadata['document_types'].get(doc_type, 0) + 1

                # ä¼šç¤¾åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
                metadata['companies'][company_code] = metadata['companies'].get(company_code, 0) + 1

                # å€‹åˆ¥æ–‡æ›¸æƒ…å ±
                doc_info = {
                    'time': data.get('time'),
                    'code': data.get('code'),
                    'company_name': data.get('company_name'),
                    'title': data.get('title'),
                    'doc_type': data.get('doc_type'),
                    'gcs_path': self._generate_gcs_path(data, date_str)
                }
                metadata['documents'].append(doc_info)

            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.json', mode='w', encoding='utf-8')
            json.dump(metadata, temp_file, ensure_ascii=False, indent=2)
            temp_file.close()

            try:
                # GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                year = date_str[:4]
                month = date_str[4:6]
                day = date_str[6:8]
                gcs_path = f"{self.base_path}/{year}/{month}/{day}/metadata_{date_str}.json"

                if self._upload_to_gcs(temp_file.name, gcs_path):
                    self.logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’GCSã«ä¿å­˜ã—ã¾ã—ãŸ: {gcs_path}")
                else:
                    self.logger.error("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®GCSä¿å­˜ã«å¤±æ•—")

            finally:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                if os.path.exists(temp_file.name):
                    os.unlink(temp_file.name)

        except Exception as e:
            self.logger.error(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    def scrape_date(self, date_str: str) -> int:
        """æŒ‡å®šæ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦GCSã«ä¿å­˜ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ç‰ˆï¼‰"""
        if not self._check_date_exists(date_str):
            self.logger.warning(f"æ—¥ä»˜ {date_str} ã®ãƒ‡ãƒ¼ã‚¿ã¯å­˜åœ¨ã—ã¾ã›ã‚“")
            return 0

        total_saved = 0
        page_index = 1
        batch_size = 50  # 50ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†

        while True:
            page_data = self.scrape_page(page_index, date_str)
            if not page_data:
                self.logger.info(f"ãƒšãƒ¼ã‚¸ {page_index} ã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’çµ‚äº†")
                break

            self.logger.info(f"å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨å‰: {len(page_data)}ä»¶ã®æ–‡æ›¸")

            # å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
            if self.company_market_map and self.excluded_markets:
                before = len(page_data)

                def _is_excluded_by_market(data: Dict) -> bool:
                    code_raw = str(data.get('code', '')).strip().upper()
                    code_norm = normalize_code(code_raw)
                    market = (
                        self.company_market_map.get(code_raw)
                        or self.company_market_map.get(code_norm)
                    )
                    is_excluded = market in self.excluded_markets
                    if is_excluded:
                        self.logger.info(f"  é™¤å¤–å¯¾è±¡: ã‚³ãƒ¼ãƒ‰={code_raw} (æ­£è¦åŒ–å¾Œ={code_norm}), å¸‚å ´={market}")
                    else:
                        self.logger.debug(f"  é™¤å¤–å¯¾è±¡å¤–: ã‚³ãƒ¼ãƒ‰={code_raw} (æ­£è¦åŒ–å¾Œ={code_norm}), å¸‚å ´={market}")
                    return is_excluded

                page_data = [d for d in page_data if not _is_excluded_by_market(d)]
                excluded = before - len(page_data)
                if excluded:
                    self.logger.info(f"å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿: {excluded}ä»¶ã‚’é™¤å¤–ï¼ˆå¯¾è±¡å¸‚å ´: {', '.join(sorted(self.excluded_markets))}ï¼‰")

                # GCSã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç›´å‰ã«ãƒ­ã‚°ã‚’è¿½åŠ 
                self.logger.info(f"GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ä»¶æ•°: {len(page_data)}ä»¶")

            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰å‡¦ç†
            if len(page_data) >= batch_size:
                self.logger.info(f"ãƒšãƒ¼ã‚¸ {page_index}: {len(page_data)}ä»¶ã‚’ãƒãƒƒãƒå‡¦ç†ä¸­...")
                saved_count = self._process_documents_cloud(page_data, date_str)
                total_saved += saved_count
                self.logger.info(f"ãƒšãƒ¼ã‚¸ {page_index} å®Œäº†: {saved_count}ä»¶ä¿å­˜")

                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                page_data.clear()

            page_index += 1

        # æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
        if page_data:
            self.logger.info(f"æœ€çµ‚ãƒãƒƒãƒ: {len(page_data)}ä»¶ã‚’å‡¦ç†ä¸­...")
            saved_count = self._process_documents_cloud(page_data, date_str)
            total_saved += saved_count
            self.logger.info(f"æœ€çµ‚ãƒãƒƒãƒå®Œäº†: {saved_count}ä»¶ä¿å­˜")

        if total_saved > 0:
            self.logger.info(f"åˆè¨ˆ {total_saved} ä»¶ã®æ–‡æ›¸ã‚’GCSã«ä¿å­˜ã—ã¾ã—ãŸ")
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯æœ€å¾Œã«ã¾ã¨ã‚ã¦ä¿å­˜
            self._save_metadata_to_gcs(self._get_all_metadata(date_str), date_str)
            self.logger.info(f"å®Œäº†: æ—¥ä»˜ {date_str}, ä¿å­˜ä»¶æ•°: {total_saved}")
            return total_saved
        else:
            self.logger.warning("å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            self.logger.info(f"å®Œäº†: æ—¥ä»˜ {date_str}, ä¿å­˜ä»¶æ•°: 0")
            return 0

    def _get_all_metadata(self, date_str: str) -> List[Dict]:
        """æŒ‡å®šæ—¥ä»˜ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ã«ä½¿ç”¨"""
        self.logger.info("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—ä¸­...")
        all_data = []
        page_index = 1

        while True:
            page_data = self.scrape_page(page_index, date_str)
            if not page_data:
                break
            all_data.extend(page_data)
            page_index += 1

        return all_data

    def run_date_range(self, start_date: str, end_date: str) -> Dict[str, int]:
        """æ—¥ä»˜ç¯„å›²ã§å®Ÿè¡Œ"""
        start_dt = datetime.strptime(start_date, '%Y%m%d')
        end_dt = datetime.strptime(end_date, '%Y%m%d')

        results = {}
        current_dt = start_dt

        while current_dt <= end_dt:
            date_str = current_dt.strftime('%Y%m%d')
            self.logger.info(f"ğŸ¯ æ—¥ä»˜ {date_str} ã®å‡¦ç†ã‚’é–‹å§‹")
            self.logger.info(f"  â€¢ å¯¾è±¡URL: https://www.release.tdnet.info/inbs/I_list_001_{date_str}.html")
            self.logger.info(f"  â€¢ å‡¦ç†äºˆå®š: ãƒšãƒ¼ã‚¸é †æ¬¡å–å¾— -> å¸‚å ´ãƒ•ã‚£ãƒ«ã‚¿ -> PDFå–å¾—ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

            try:
                count = self.scrape_date(date_str)
                results[date_str] = count
            except Exception as e:
                self.logger.error(f"æ—¥ä»˜ {date_str} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                results[date_str] = -1

            current_dt += timedelta(days=1)

        return results


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='TDnet Cloud Scraper')
    parser.add_argument('--date', type=str, help='å¯¾è±¡æ—¥ä»˜ (YYYYMMDDå½¢å¼)')
    parser.add_argument('--start-date', type=str, help='é–‹å§‹æ—¥ä»˜ (YYYYMMDDå½¢å¼)')
    parser.add_argument('--end-date', type=str, help='çµ‚äº†æ—¥ä»˜ (YYYYMMDDå½¢å¼)')
    parser.add_argument('--single-thread', action='store_true', help='ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--config', type=str, default='config/config.yaml', help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')

    args = parser.parse_args()

    # æ—¥ä»˜æŒ‡å®šã®æ¤œè¨¼
    if args.date and (args.start_date or args.end_date):
        print("ã‚¨ãƒ©ãƒ¼: --date ã¨ --start-date/--end-date ã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
        return 1

    if args.start_date and not args.end_date:
        print("ã‚¨ãƒ©ãƒ¼: --start-date ã‚’æŒ‡å®šã™ã‚‹å ´åˆã¯ --end-date ã‚‚å¿…è¦ã§ã™")
        return 1

    if args.end_date and not args.start_date:
        print("ã‚¨ãƒ©ãƒ¼: --end-date ã‚’æŒ‡å®šã™ã‚‹å ´åˆã¯ --start-date ã‚‚å¿…è¦ã§ã™")
        return 1

    try:
        scraper = TDNetCloud(args.config, use_multithread=not args.single_thread)

        if args.date:
            # å˜ä¸€æ—¥ä»˜ã®å‡¦ç†
            count = scraper.scrape_date(args.date)
            print(f"å‡¦ç†å®Œäº†: {count} ä»¶ã®æ–‡æ›¸ã‚’ä¿å­˜")

        elif args.start_date and args.end_date:
            # æ—¥ä»˜ç¯„å›²ã®å‡¦ç†
            results = scraper.run_date_range(args.start_date, args.end_date)
            total_count = sum(count for count in results.values() if count > 0)
            print(f"ç¯„å›²å‡¦ç†å®Œäº†: åˆè¨ˆ {total_count} ä»¶ã®æ–‡æ›¸ã‚’ä¿å­˜")
            for date, count in results.items():
                print(f"  {date}: {count} ä»¶")
        else:
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰æ—¥ä»˜ã‚’å–å¾—ã€ãªã‘ã‚Œã°ä»Šæ—¥ã®æ—¥ä»˜
            import os
            target_date = os.environ.get('TARGET_DATE')

            if target_date:
                print(f"ç’°å¢ƒå¤‰æ•°ã‹ã‚‰æ—¥ä»˜({target_date})ã‚’å–å¾—ã—ã¦å®Ÿè¡Œã—ã¾ã™")
                count = scraper.scrape_date(target_date)
                print(f"å‡¦ç†å®Œäº†: {count} ä»¶ã®æ–‡æ›¸ã‚’ä¿å­˜")
            else:
                # ä»Šæ—¥ã®æ—¥ä»˜ã§è‡ªå‹•å®Ÿè¡Œ
                import pytz
                jst = pytz.timezone('Asia/Tokyo')
                today = datetime.now(jst).strftime('%Y%m%d')
                print(f"ä»Šæ—¥ã®æ—¥ä»˜({today})ã§è‡ªå‹•å®Ÿè¡Œã—ã¾ã™")
                count = scraper.scrape_date(today)
                print(f"å‡¦ç†å®Œäº†: {count} ä»¶ã®æ–‡æ›¸ã‚’ä¿å­˜")

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
